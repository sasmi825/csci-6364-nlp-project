{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "438b62cb",
   "metadata": {},
   "source": [
    "# CSCI-6364 Final Project\n",
    "# Data \n",
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a29b374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3cf677c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"./data/train.jsonl\", lines=True)\n",
    "df_val = pd.read_json(\"./data/validation.jsonl\", lines=True)\n",
    "df_test = pd.read_json(\"./data/test.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02083e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label label_text\n",
       "0                            i didnt feel humiliated      0    sadness\n",
       "1  i can go from feeling so hopeless to so damned...      0    sadness\n",
       "2   im grabbing a minute to post i feel greedy wrong      3      anger\n",
       "3  i am ever feeling nostalgic about the fireplac...      2       love\n",
       "4                               i am feeling grouchy      3      anger"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5659e62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 16000,\n",
      "validation samples: 2000\n",
      "Test Samples: 2000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training samples: {len(df)},\\nvalidation samples: {len(df_val)}\\nTest Samples: {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4809da7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target variable distribution in training set: \n",
      "{'joy': 5362, 'sadness': 4666, 'anger': 2159, 'fear': 1937, 'love': 1304, 'surprise': 572}\n"
     ]
    }
   ],
   "source": [
    "print(\"Target variable distribution in training set: \")\n",
    "print(df.label_text.value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da7ad80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels \n",
      "['sadness' 'anger' 'love' 'surprise' 'fear' 'joy']\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique labels \")\n",
    "print(df.label_text.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1ecc01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "716d1316",
   "metadata": {},
   "source": [
    "## Data Pre-Processing \n",
    "### Bag of Words and TF-IDF representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "457ecd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57993cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/diadochus/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/diadochus/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e8e23c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f08b65bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# words are converted into shorten root words to remove redundancy\n",
    "# STEMMER = PorterStemmer()\n",
    "# Stemming produces Intermediate representation of word. It may or may not return meaningful word.\n",
    "LEMMATIZER = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "239ab91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sentence: str):\n",
    "    # convert to lower case and tokenize words\n",
    "    word_tokens = word_tokenize(sentence.lower())\n",
    "    # remove stop words from the sentence.\n",
    "    filtered_sentence = [w for w in word_tokens if not w in STOP_WORDS]\n",
    "    # Stemming\n",
    "    # filtered_sentence = [STEMMER.stem(w) for w in filtered_sentence]\n",
    "    # Lemmatization\n",
    "    filtered_sentence = [LEMMATIZER.lemmatize(w) for w in filtered_sentence]\n",
    "    \n",
    "    return \" \".join(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc417768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unprocessed text: \n",
      "\n",
      " i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake\n"
     ]
    }
   ],
   "source": [
    "# input text\n",
    "print(\"Unprocessed text: \\n\\n\", df.text.iloc[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e59eb539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed text: \n",
      "\n",
      " go feeling hopeless damned hopeful around someone care awake\n"
     ]
    }
   ],
   "source": [
    "# processed text\n",
    "print(\"Processed text: \\n\\n\", preprocessing(df.text.iloc[1])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51dae610",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(processed_text=df.text.apply(lambda x: preprocessing(x)))\n",
    "df_val = df_val.assign(processed_text=df_val.text.apply(lambda x: preprocessing(x)))\n",
    "df_test = df_test.assign(processed_text=df_test.text.apply(lambda x: preprocessing(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67c9f3a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_text</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "      <td>didnt feel humiliated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "      <td>go feeling hopeless damned hopeful around some...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "      <td>im grabbing minute post feel greedy wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "      <td>love</td>\n",
       "      <td>ever feeling nostalgic fireplace know still pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "      <td>feeling grouchy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label label_text  \\\n",
       "0                            i didnt feel humiliated      0    sadness   \n",
       "1  i can go from feeling so hopeless to so damned...      0    sadness   \n",
       "2   im grabbing a minute to post i feel greedy wrong      3      anger   \n",
       "3  i am ever feeling nostalgic about the fireplac...      2       love   \n",
       "4                               i am feeling grouchy      3      anger   \n",
       "\n",
       "                                      processed_text  \n",
       "0                              didnt feel humiliated  \n",
       "1  go feeling hopeless damned hopeful around some...  \n",
       "2          im grabbing minute post feel greedy wrong  \n",
       "3  ever feeling nostalgic fireplace know still pr...  \n",
       "4                                    feeling grouchy  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b44528",
   "metadata": {},
   "source": [
    "### Simple Features: Bag of Words and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fa8b694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2ab7168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_features(\n",
    "    vectorizer,  # Bag of words or TF-IDF vectorizer\n",
    "    df_train,\n",
    "    df_validation,\n",
    "    df_test,\n",
    "    text_col=\"processed_text\", \n",
    "    label_col=\"label\"\n",
    ") -> dict:\n",
    "    \"\"\"Transforms text data to BOW or TF-IDF representation.\"\"\"\n",
    "    # fit and transform training data\n",
    "    X = vectorizer.fit_transform(df_train[text_col].tolist()) \n",
    "    X_train = X.toarray()\n",
    "    y_train = df_train[label_col].to_numpy()\n",
    "    \n",
    "    # generate validation data\n",
    "    X_val = vectorizer.transform(df_validation[text_col].tolist())\n",
    "    X_val = X_val.toarray()\n",
    "    y_val = df_validation[label_col].to_numpy()\n",
    "    \n",
    "    # generate test data\n",
    "    X_test = vectorizer.transform(df_test[text_col].tolist())\n",
    "    X_test = X_test.toarray()\n",
    "    y_test = df_test[label_col].to_numpy()\n",
    "    \n",
    "    return_dict = {\n",
    "        \"train\": {\"x\": X_train, \"y\": y_train},\n",
    "        \"validation\": {\"x\": X_val, \"y\": y_val},\n",
    "        \"test\": {\"x\": X_test, \"y\": y_test},\n",
    "    }\n",
    "    \n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89e5a4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Bag-of-Words representations\n",
    "bow_dataset = get_simple_features(\n",
    "    vectorizer=CountVectorizer(),\n",
    "    df_train=df,\n",
    "    df_validation=df_val,\n",
    "    df_test=df_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1060b08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Bag-of-Words representations\n",
    "tfidf_dataset = get_simple_features(\n",
    "    vectorizer=TfidfVectorizer(),\n",
    "    df_train=df,\n",
    "    df_validation=df_val,\n",
    "    df_test=df_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "293b557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data as pickle files.\n",
    "import pickle\n",
    "\n",
    "with open(\"./data/bow.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(bow_dataset, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(\"./data/tfidf.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(tfidf_dataset, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#with open(\"./data/bow.pickle\", \"rb\") as handle:\n",
    "#    bow_dataset = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d68b5ab",
   "metadata": {},
   "source": [
    "### Embedding Features: get sentence embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1670062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "056759c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_embedding_features(\n",
    "    df_train,\n",
    "    df_validation,\n",
    "    df_test,\n",
    "    text_col=\"text\", \n",
    "    label_col=\"label\"\n",
    ") -> dict:\n",
    "    # initialize the model \n",
    "    vectorizer = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    \n",
    "    \"\"\"Transforms text data to BOW or TF-IDF representation.\"\"\"\n",
    "    # fit and transform training data\n",
    "    X_train = vectorizer.encode(df_train[text_col].tolist()) \n",
    "    y_train = df_train[label_col].to_numpy()\n",
    "    \n",
    "    # generate validation data\n",
    "    X_val = vectorizer.encode(df_validation[text_col].tolist())\n",
    "    y_val = df_validation[label_col].to_numpy()\n",
    "    \n",
    "    # generate test data\n",
    "    X_test = vectorizer.encode(df_test[text_col].tolist())\n",
    "    y_test = df_test[label_col].to_numpy()\n",
    "    \n",
    "    return_dict = {\n",
    "        \"train\": {\"x\": X_train, \"y\": y_train},\n",
    "        \"validation\": {\"x\": X_val, \"y\": y_val},\n",
    "        \"test\": {\"x\": X_test, \"y\": y_test},\n",
    "    }\n",
    "    \n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6499c2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Bag-of-Words representations\n",
    "semb_dataset = get_sent_embedding_features(\n",
    "    df_train=df,\n",
    "    df_validation=df_val,\n",
    "    df_test=df_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95255d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data as pickle file\n",
    "import pickle\n",
    "\n",
    "with open(\"./data/sent_emb.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(semb_dataset, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5de2dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70af3966",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93bd4db",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6266378a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.metrics import classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d9b5c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.23      0.29       550\n",
      "           1       0.54      0.23      0.32       704\n",
      "           2       0.18      0.42      0.25       178\n",
      "           3       0.31      0.31      0.31       275\n",
      "           4       0.21      0.38      0.27       212\n",
      "           5       0.11      0.42      0.17        81\n",
      "\n",
      "    accuracy                           0.28      2000\n",
      "   macro avg       0.29      0.33      0.27      2000\n",
      "weighted avg       0.38      0.28      0.29      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(tfidf_dataset[\"train\"][\"x\"], tfidf_dataset[\"train\"][\"y\"])\n",
    "nb_predictions = clf.predict(tfidf_dataset[\"validation\"][\"x\"])\n",
    "print(classification_report(tfidf_dataset[\"validation\"][\"y\"], nb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51256a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.94      0.76       550\n",
      "           1       0.71      0.93      0.80       704\n",
      "           2       0.68      0.10      0.17       178\n",
      "           3       0.88      0.45      0.60       275\n",
      "           4       0.85      0.41      0.55       212\n",
      "           5       0.00      0.00      0.00        81\n",
      "\n",
      "    accuracy                           0.70      2000\n",
      "   macro avg       0.63      0.47      0.48      2000\n",
      "weighted avg       0.70      0.70      0.65      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = BernoulliNB()\n",
    "clf.fit(tfidf_dataset[\"train\"][\"x\"], tfidf_dataset[\"train\"][\"y\"])\n",
    "nb_predictions = clf.predict(tfidf_dataset[\"validation\"][\"x\"])\n",
    "print(classification_report(tfidf_dataset[\"validation\"][\"y\"], nb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9b58f64",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.91      0.76       581\n",
      "           1       0.71      0.96      0.81       695\n",
      "           2       0.79      0.09      0.17       159\n",
      "           3       0.82      0.45      0.58       275\n",
      "           4       0.78      0.29      0.42       224\n",
      "           5       0.00      0.00      0.00        66\n",
      "\n",
      "    accuracy                           0.70      2000\n",
      "   macro avg       0.63      0.45      0.46      2000\n",
      "weighted avg       0.70      0.70      0.65      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diadochus/anaconda3/envs/hug/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/diadochus/anaconda3/envs/hug/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/diadochus/anaconda3/envs/hug/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "nb_predictions = clf.predict(tfidf_dataset[\"test\"][\"x\"])\n",
    "print(classification_report(tfidf_dataset[\"test\"][\"y\"], nb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549d6162",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f448358",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32edd0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1229bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Hyper-parameters.\n",
    "kernels = [\"linear\", \"poly\", \"rbf\", \"sigmoid\", \"precomputed\"]\n",
    "Cvalues = [1E-1, 1E0, 1E1, 1E2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6079e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search for Bag of words\n",
    "results = {\"kernel\": [], \"C\": [], \"F1\": []}\n",
    "for kernel in kernels:\n",
    "    for c in Cvalues:\n",
    "        print(f\"kernel: {kernel}, C: {c}\")\n",
    "        results[\"kernel\"].append(kernel)\n",
    "        results[\"C\"].append(c)\n",
    "        \n",
    "        model = svm.SVC(kernel=kernel, C=c).fit(bow_dataset[\"train\"][\"x\"], bow_dataset[\"train\"][\"y\"])\n",
    "        predict_dev = model.predict(bow_dataset[\"validation\"][\"x\"])\n",
    "        \n",
    "        results[\"F1\"].append(f1_score(bow_dataset[\"validation\"][\"y\"], predict_dev, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb305d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate BoW SVM models\n",
    "dfr = pd.DataFrame.from_dict(results)\n",
    "dfr = dfr.sort_values(by=\"F1\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "best_kernel = dfr.kernel.tolist()[0]\n",
    "best_c = dfr.C.tolist()[0]\n",
    "model = svm.SVC(kernel=best_kernel, C=best_c).fit(bow_dataset[\"train\"][\"x\"], bow_dataset[\"train\"][\"y\"])\n",
    "predict_test = model.predict(bow_dataset[\"test\"][\"x\"])\n",
    "        \n",
    "print(\"Best F1 score: \", f1_score(bow_dataset[\"test\"][\"y\"], predict_test, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ca9556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search for TFIDF\n",
    "results_tfidf = {\"kernel\": [], \"C\": [], \"F1\": []}\n",
    "for kernel in kernels:\n",
    "    for c in Cvalues:\n",
    "        print(f\"kernel: {kernel}, C: {c}\")\n",
    "        results_tfidf[\"kernel\"].append(kernel)\n",
    "        results_tfidf[\"C\"].append(c)\n",
    "        \n",
    "        model = svm.SVC(kernel=kernel, C=c).fit(tfidf_dataset[\"train\"][\"x\"], tfidf_dataset[\"train\"][\"y\"])\n",
    "        predict_dev = model.predict(tfidf_dataset[\"validation\"][\"x\"])\n",
    "        \n",
    "        results_tfidf[\"F1\"].append(f1_score(tfidf_dataset[\"validation\"][\"y\"], predict_dev, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089d80bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate TF-IDF SVM models\n",
    "dfr = pd.DataFrame.from_dict(results_tfidf)\n",
    "dfr = dfr.sort_values(by=\"F1\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "best_kernel = dfr.kernel.tolist()[0]\n",
    "best_c = dfr.C.tolist()[0]\n",
    "model = svm.SVC(kernel=best_kernel, C=best_c).fit(bow_dataset[\"train\"][\"x\"], bow_dataset[\"train\"][\"y\"])\n",
    "predict_test = model.predict(bow_dataset[\"test\"][\"x\"])\n",
    "        \n",
    "print(\"Best F1 score: \", f1_score(bow_dataset[\"test\"][\"y\"], predict_test, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b67c94a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e8b8ca6",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Pytorch: [Tutorial](https://pytorch.org/tutorials/beginner/introyt/trainingyt.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02b5f243",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diadochus/anaconda3/envs/hug/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.autograd import Variable\n",
    "from torcheval.metrics import MulticlassF1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d31a09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import uuid\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d285b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "MAX_EPOCHS = 100\n",
    "NUM_CLASSES = df.label.nunique()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0556b3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir_if_not_exists(path):\n",
    "    \"\"\"Checks if a directory exists. If not, creates it.\n",
    "\n",
    "    Args:\n",
    "        path: The path to the directory (including subdirectories).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa934694",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-class Logistic Regression model in PyTorch.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        \"\"\"\n",
    "        Initialize the model with linear layer and softmax activation.\n",
    "\n",
    "        Args:\n",
    "          in_features: Number of input features.\n",
    "          out_features: Number of output features.\n",
    "        \"\"\"\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        # self.init_weights()\n",
    "        \n",
    "    #def init_weights(self) -> None:\n",
    "    #    initrange = 0.1\n",
    "    #    self.linear.bias.data.zero_()\n",
    "    #    self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform the forward pass: linear transformation.\n",
    "\n",
    "        Args:\n",
    "          x: Input data (tensor).\n",
    "\n",
    "        Returns:\n",
    "          Logits (tensor): Output of the linear layer before activation.\n",
    "        \"\"\"\n",
    "        logits = self.linear(x)\n",
    "        return logits  # F.log_softmax(logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04d72fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, loss_fn, optimizer, epoch_index, verbose=False):  # tb_writer\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights or update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            last_loss = running_loss / 100  # loss per batch\n",
    "            if verbose:\n",
    "                print('\\tbatch {} loss: {}'.format(i + 1, last_loss))\n",
    "            #tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            #tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "            \n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7679bc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_step(model, validation_loader, loss_fn):\n",
    "    metric = MulticlassF1Score(num_classes=NUM_CLASSES, average=\"macro\")\n",
    "    running_vloss = 0.0\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(validation_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            vinputs = vinputs.to(device)\n",
    "            vlabels = vlabels.to(device)\n",
    "            voutputs = model(vinputs)\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            running_vloss += vloss.item()\n",
    "            metric.update(voutputs, vlabels)\n",
    "            \n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    vf1_macro = metric.compute()\n",
    "    return avg_vloss, vf1_macro.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6de2ac12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stopping_flag(arr, patience=3):\n",
    "    \"\"\"\n",
    "    Checks if the last 'patience' no of elements in a list are all greater than the \n",
    "    (patience+1)th element from the last.\n",
    "\n",
    "    Args:\n",
    "        my_list: A list of numbers.\n",
    "\n",
    "    Returns:\n",
    "        bool flag.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(arr) <= patience:\n",
    "        return False  # List must have at least \"patience\" elements\n",
    "    \n",
    "    comparison_value = arr[-(patience+1)]\n",
    "    return all(val > comparison_value for val in arr[-patience:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef48f096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "#timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "#writer = SummaryWriter('runs/logistic_{}'.format(timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb7bbc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, train_loader, validation_loader, model_save_file, lr=1e-2, patience=3, verbose=False):\n",
    "    # Loss fn; it takes logits and labels.\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    # Optimizers specified in the torch.optim package\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, lr, gamma=0.95)\n",
    "    \n",
    "    best_vloss = 1_000_000.\n",
    "    epoch_number = 0\n",
    "    output_json = {\n",
    "        \"patience\": patience,\n",
    "        \"lr\": lr,\n",
    "        \"model_save_path\": f\"./models/{model_save_file}.pth\",\n",
    "        \"training_loss\": [],\n",
    "        \"validation_loss\": [],\n",
    "        \"validation_f1\": [],\n",
    "    }\n",
    "    for epoch in (pbar := tqdm(range(MAX_EPOCHS))):\n",
    "        if verbose:\n",
    "            print('EPOCH {}:'.format(epoch_number + 1))\n",
    "        # Make sure gradient tracking is on, and do a pass over the data\n",
    "        model.train(True)\n",
    "        avg_loss = train_one_epoch(\n",
    "            model, \n",
    "            train_loader, \n",
    "            loss_fn, \n",
    "            optimizer, \n",
    "            epoch_number,\n",
    "            verbose=verbose\n",
    "        )  # writer\n",
    "\n",
    "        # Set the model to evaluation mode, disabling dropout and using population\n",
    "        # statistics for batch normalization.\n",
    "        model.eval()\n",
    "        avg_vloss, vf1_macro = validation_step(model, validation_loader, loss_fn)\n",
    "        if verbose:\n",
    "            print('LOSS train {} valid {} valid F1 {}'.format(avg_loss, avg_vloss, vf1_macro))\n",
    "\n",
    "        # Learning rate scheduling should be applied after optimizerâ€™s update\n",
    "        # use the following template to refer to schedulers algorithms\n",
    "        #    train(...), validation(...), scheduler.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # add metrics to output json\n",
    "        output_json[\"training_loss\"].append(avg_loss)\n",
    "        output_json[\"validation_loss\"].append(avg_vloss)\n",
    "        output_json[\"validation_f1\"].append(vf1_macro)\n",
    "        \n",
    "        # early stopping\n",
    "        es_flag = early_stopping_flag(output_json[\"validation_loss\"], patience=patience)\n",
    "        if es_flag:\n",
    "            return output_json\n",
    "        \n",
    "        # Log the running loss averaged per batch\n",
    "        # for both training and validation\n",
    "        #writer.add_scalars('Training vs. Validation Loss',\n",
    "        #                { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "        #                epoch_number + 1)\n",
    "        #writer.flush()\n",
    "\n",
    "        # Track best performance, and save the model's state\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            model_path = output_json[\"model_save_path\"]\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        \n",
    "        #pbar.set_description(f\"Epoch {epoch_number+1}\")\n",
    "        pbar.set_postfix(tl=round(avg_loss, 2), vl=round(avg_vloss, 2))\n",
    "        #pbar.write(f\"LOSS train {avg_loss} valid {avg_vloss}\")\n",
    "\n",
    "        epoch_number += 1\n",
    "    \n",
    "    return output_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dda88d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_type, train_batch_size=32):\n",
    "    with open(f\"./data/{data_type}.pickle\", \"rb\") as handle:\n",
    "        pickled_dataset = pickle.load(handle)\n",
    "\n",
    "    train_samples = TensorDataset(  # tfidf_dataset, bow_dataset\n",
    "        torch.tensor(pickled_dataset[\"train\"][\"x\"]).type(torch.FloatTensor),  # LongTensor\n",
    "        torch.tensor(pickled_dataset[\"train\"][\"y\"])\n",
    "        # F.one_hot(torch.tensor(bow_dataset[\"train\"][\"y\"])).type(torch.FloatTensor)\n",
    "    )\n",
    "\n",
    "    valid_samples = TensorDataset(\n",
    "        torch.tensor(pickled_dataset[\"validation\"][\"x\"]).type(torch.FloatTensor),\n",
    "        torch.tensor(pickled_dataset[\"validation\"][\"y\"])\n",
    "        # F.one_hot(torch.tensor(bow_dataset[\"validation\"][\"y\"])).type(torch.FloatTensor)\n",
    "    )\n",
    "\n",
    "    test_samples = TensorDataset(\n",
    "        torch.tensor(pickled_dataset[\"test\"][\"x\"]).type(torch.FloatTensor),\n",
    "        torch.tensor(pickled_dataset[\"test\"][\"y\"])\n",
    "        # F.one_hot(torch.tensor(bow_dataset[\"test\"][\"y\"]))\n",
    "    )\n",
    "\n",
    "    # We wrap train_samples into a pytorch DataLoader\n",
    "    train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size, pin_memory=True)\n",
    "    validation_dataloader = DataLoader(valid_samples, batch_size=train_batch_size, pin_memory=True)\n",
    "    test_dataloader = DataLoader(test_samples, pin_memory=True)\n",
    "    \n",
    "    nfeatures = np.shape(pickled_dataset[\"train\"][\"x\"])[-1]\n",
    "    \n",
    "    return train_dataloader, validation_dataloader, test_dataloader, nfeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c09a9034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a given set of hyper-parameters, trains the logistic regression model.\n",
    "# saves the best model checkpoint (one with low validation loss) and training log file\n",
    "def run_logistic_one_config(model_name, data_type, lr):\n",
    "    # Train the model\n",
    "    # Model and data parameters\n",
    "    param_json = {\n",
    "        \"run_id\": str(uuid.uuid4()).replace(\"-\", \"_\"),\n",
    "        \"model_name\": model_name,\n",
    "        \"data_type\": data_type,\n",
    "    }\n",
    "    model_save_file = f\"{param_json[\"model_name\"]}_{param_json[\"run_id\"]}\"\n",
    "\n",
    "    # get dataset in torch dataloader format\n",
    "    train_dataloader, validation_dataloader, test_dataloader, nfeatures = get_data(\n",
    "        param_json[\"data_type\"], \n",
    "        train_batch_size=32\n",
    "    )\n",
    "\n",
    "    # initialize model\n",
    "    model = LogisticRegression(\n",
    "        in_features=nfeatures, \n",
    "        out_features=NUM_CLASSES\n",
    "    )\n",
    "    model.to(device)\n",
    "    # print(model)\n",
    "\n",
    "    # run the training loop\n",
    "    run_json = fit(\n",
    "        model, \n",
    "        train_dataloader, \n",
    "        validation_dataloader, \n",
    "        model_save_file,\n",
    "        lr=lr,\n",
    "        patience=3, \n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    for k, v in run_json.items():\n",
    "        param_json[k] = v\n",
    "\n",
    "    # save run logs\n",
    "    log_directory = f\"./logs/{param_json[\"model_name\"]}_{param_json[\"data_type\"]}\"\n",
    "    create_dir_if_not_exists(log_directory)\n",
    "    logfilename = f\"{log_directory}/{param_json[\"run_id\"]}.json\"\n",
    "    with open(logfilename, \"w\") as f:\n",
    "        json.dump(param_json, f, indent=4)\n",
    "        \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3c8cefb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search for best hyper-parameters.\n",
    "model_name = \"logistic\" \n",
    "dtypes = [\"bow\", \"tfidf\", \"sent_emb\"]\n",
    "lrs = [1e-1, 1e-2, 1e-3]\n",
    "\n",
    "for data_type in dtypes:\n",
    "    for lr in lrs:\n",
    "        # print(model_name, data_type, nlayers, dropout, lr)\n",
    "        run_logistic_one_config(model_name, data_type, lr)\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67f2fb8",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22c2cf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2057ed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model_path(mypath, return_df=False):\n",
    "    # read all logfiles\n",
    "    onlyfiles = [f\"{mypath}{f}\" for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "    logs = []\n",
    "    for fn in onlyfiles:\n",
    "        with open(fn, \"r\") as f:\n",
    "            logs.append(json.load(f))\n",
    "    dflogs = pd.DataFrame(logs)\n",
    "\n",
    "    # extract macro F1 score for each run\n",
    "    bestf1s = []\n",
    "    for _, row in dflogs.iterrows():\n",
    "        p = row[\"patience\"]\n",
    "        bestf1 = row[\"validation_f1\"][-1*(p+1)]\n",
    "        bestf1s.append(bestf1)\n",
    "\n",
    "    dflogs = dflogs.assign(best_f1=bestf1s)\n",
    "    \n",
    "    # get model path with best macro F1 score.\n",
    "    mpath = dflogs.sort_values(by=\"best_f1\", ascending=False).reset_index(drop=True).model_save_path.iloc[0]\n",
    "    if return_df:\n",
    "        return mpath, dflogs\n",
    "    else:\n",
    "        return mpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28b343e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_truth_labels(model, test_dataloader):\n",
    "    # gather predictions and truth labels.\n",
    "    pred_labels = []\n",
    "    truth_labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, tdata in enumerate(test_dataloader):\n",
    "            tinputs, tlabels = tdata\n",
    "            voutputs = model(tinputs)\n",
    "            tmptruth = tlabels.detach().numpy()[0]\n",
    "            truth_labels.append(tmptruth)\n",
    "            tmpreds = np.argmax(voutputs.detach().numpy())\n",
    "            pred_labels.append(tmpreds)\n",
    "    return truth_labels, pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8efba35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"logistic\"\n",
    "data_type = \"sent_emb\"\n",
    "\n",
    "mypath = f\"./logs/{model_type}_{data_type}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e71cc9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model path with best macro F1 score.\n",
    "mpath = get_best_model_path(mypath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "43b870a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset in torch dataloader format\n",
    "_, _, test_dataloader, nfeatures = get_data(\n",
    "    data_type, \n",
    "    train_batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "aff8025e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize model\n",
    "model = LogisticRegression(\n",
    "    in_features=nfeatures, \n",
    "    out_features=NUM_CLASSES\n",
    ")\n",
    "# load the saved weights\n",
    "model.load_state_dict(torch.load(mpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "97de99ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_labels, pred_labels = get_predictions_truth_labels(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "72c7e43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.76      0.74       581\n",
      "           1       0.72      0.79      0.75       695\n",
      "           2       0.49      0.41      0.44       159\n",
      "           3       0.67      0.56      0.61       275\n",
      "           4       0.64      0.61      0.63       224\n",
      "           5       0.52      0.38      0.44        66\n",
      "\n",
      "    accuracy                           0.69      2000\n",
      "   macro avg       0.63      0.58      0.60      2000\n",
      "weighted avg       0.68      0.69      0.68      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report for Multi-class Logistic regression.\n",
    "print(classification_report(truth_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da73c1a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a61a1a8",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e0856ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates a list of list [[inp, out], [...]]; each element contains no of input features\n",
    "# and output features for each linear layer.\n",
    "def generate_no_neurons(num1, num2, n):\n",
    "    if num1 <= num2:\n",
    "        raise ValueError(\"num1 must be greater than num2\")\n",
    "        \n",
    "    if n == 1:\n",
    "        return [[num1, num2]]\n",
    "    \n",
    "    num1b = int(round(1.3*num1, 0))\n",
    "    # Calculate the base of the exponential function\n",
    "    base = (num2 / num1b) ** (1 / (n))\n",
    "\n",
    "    # Generate the sequence\n",
    "    sequence = [num1b * base**i for i in range(n-1)]\n",
    "    sequence = [int(round(x, 0)) for x in sequence]\n",
    "    sequence = [num1] + sequence + [num2]\n",
    "    output = []\n",
    "    for i in range(len(sequence)-1):\n",
    "        output.append([sequence[i], sequence[i+1]])\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "579d8eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaNeuralNetworks(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-class Neural Network in PyTorch.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, nlayers, dropout):\n",
    "        \"\"\"\n",
    "        Initialize the model with linear layers.\n",
    "\n",
    "        Args:\n",
    "          in_features: Number of input features.\n",
    "          out_features: Number of output features.\n",
    "          nlayers: no of layers in the neural network.\n",
    "          dropout: dropout probability.\n",
    "        \"\"\"\n",
    "        super(VanillaNeuralNetworks, self).__init__()\n",
    "        \n",
    "        layer_info = generate_no_neurons(in_features, out_features, nlayers)\n",
    "        \n",
    "        layers = []\n",
    "        for lay in layer_info[:-1]:\n",
    "            in_, out_ = lay\n",
    "            layers.append(nn.Linear(in_, out_))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.BatchNorm1d(out_))\n",
    "            layers.append(nn.Dropout(p=dropout))\n",
    "        \n",
    "        self.linear_relu_stack = nn.Sequential(*layers)\n",
    "        self.output_layer = nn.Linear(layer_info[-1][0], out_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform the forward pass.\n",
    "\n",
    "        Args:\n",
    "          x: Input data (tensor).\n",
    "\n",
    "        Returns:\n",
    "          Logits (tensor): Output of the linear layer before activation.\n",
    "        \"\"\"\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        logits = self.output_layer(logits)\n",
    "        return logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6f08c6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a given set of hyper-parameters, trains the neural network model.\n",
    "# saves the best model checkpoint (one with low validation loss) and training log file\n",
    "def run_nn_one_config(model_name, data_type, nlayers, dropout, lr):\n",
    "    # Train the model\n",
    "    # Model and data parameters\n",
    "    param_json = {\n",
    "        \"run_id\": str(uuid.uuid4()).replace(\"-\", \"_\"),\n",
    "        \"model_name\": model_name,\n",
    "        \"data_type\": data_type,\n",
    "        \"nlayers\": nlayers,\n",
    "        \"dropout\": dropout,\n",
    "    }\n",
    "    model_save_file = f\"{param_json[\"model_name\"]}_{param_json[\"run_id\"]}\"\n",
    "\n",
    "    # get dataset in torch dataloader format\n",
    "    train_dataloader, validation_dataloader, test_dataloader, nfeatures = get_data(\n",
    "        param_json[\"data_type\"], \n",
    "        train_batch_size=32\n",
    "    )\n",
    "\n",
    "    # initialize model\n",
    "    model = VanillaNeuralNetworks(\n",
    "        in_features=nfeatures, \n",
    "        out_features=NUM_CLASSES,\n",
    "        nlayers=param_json[\"nlayers\"], \n",
    "        dropout=param_json[\"dropout\"]\n",
    "    )\n",
    "    model.to(device)\n",
    "    # print(model)\n",
    "\n",
    "    # run the training loop\n",
    "    run_json = fit(\n",
    "        model, \n",
    "        train_dataloader, \n",
    "        validation_dataloader, \n",
    "        model_save_file,\n",
    "        lr=lr,\n",
    "        patience=3, \n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    for k, v in run_json.items():\n",
    "        param_json[k] = v\n",
    "\n",
    "    # save run logs\n",
    "    log_directory = f\"./logs/{param_json[\"model_name\"]}_{param_json[\"data_type\"]}\"\n",
    "    create_dir_if_not_exists(log_directory)\n",
    "    logfilename = f\"{log_directory}/{param_json[\"run_id\"]}.json\"\n",
    "    with open(logfilename, \"w\") as f:\n",
    "        json.dump(param_json, f, indent=4)\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e29a3d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search for best hyper-parameters.\n",
    "model_name = \"nn\" \n",
    "dtypes = [\"bow\", \"tfidf\", \"sent_emb\"]\n",
    "nl = [2, 3, 4, 5]\n",
    "dprobs = [0.1*x for x in range(1, 6)]\n",
    "lrs = [1e-1, 1e-2, 1e-3]\n",
    "\n",
    "for data_type in dtypes:\n",
    "    for nlayers in nl:\n",
    "        for dropout in dprobs:\n",
    "            for lr in lrs:\n",
    "                # print(model_name, data_type, nlayers, dropout, lr)\n",
    "                run_nn_one_config(model_name, data_type, nlayers, dropout, lr)\n",
    "                torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6835b553",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ca152e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"nn\"\n",
    "data_type = \"sent_emb\"\n",
    "\n",
    "mypath = f\"./logs/{model_type}_{data_type}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2344c963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model path with best macro F1 score.\n",
    "mpath, dfnnlogs = get_best_model_path(mypath, return_df=True)\n",
    "model_layers = dfnnlogs.loc[dfnnlogs.model_save_path==mpath].nlayers.tolist()[0]\n",
    "model_dropout = dfnnlogs.loc[dfnnlogs.model_save_path==mpath].dropout.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9b62443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset in torch dataloader format\n",
    "_, _, test_dataloader, nfeatures = get_data(\n",
    "    data_type, \n",
    "    train_batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "12000c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize model\n",
    "model = VanillaNeuralNetworks(\n",
    "    in_features=nfeatures, \n",
    "    out_features=NUM_CLASSES,\n",
    "    nlayers=model_layers,\n",
    "    dropout=model_dropout\n",
    ")\n",
    "# load the saved weights\n",
    "model.load_state_dict(torch.load(mpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "468350d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_labels, pred_labels = get_predictions_truth_labels(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "82e62d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.78      0.77       581\n",
      "           1       0.75      0.82      0.78       695\n",
      "           2       0.63      0.48      0.54       159\n",
      "           3       0.70      0.68      0.69       275\n",
      "           4       0.69      0.67      0.68       224\n",
      "           5       0.58      0.38      0.46        66\n",
      "\n",
      "    accuracy                           0.73      2000\n",
      "   macro avg       0.69      0.63      0.66      2000\n",
      "weighted avg       0.73      0.73      0.73      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification report for Multi-layer Neural Networks\n",
    "print(classification_report(truth_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a65f7ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "321daab6",
   "metadata": {},
   "source": [
    "## Fine-Tuning Approaches\n",
    "## roBERTa Fine-tuning\n",
    "\n",
    "Huggingface fine-tuning [tutorial](https://huggingface.co/docs/transformers/training#train)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ffadb498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import torch\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import RobertaModel, RobertaTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9e35120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates pytorch dataset object for emotional data\n",
    "def return_datasets(df):\n",
    "    dset = Dataset.from_dict(\n",
    "        {\n",
    "            \"label\": df.label.tolist(),\n",
    "            \"text\": df.text.tolist(),\n",
    "        }\n",
    "    )\n",
    "    return dset\n",
    "\n",
    "\n",
    "# computes the validation macro F1 score. \n",
    "metric = evaluate.load(\"f1\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "385cee9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = DatasetDict(\n",
    "    {\n",
    "        \"train\": return_datasets(df),\n",
    "        \"validation\": return_datasets(df_val),\n",
    "        \"test\": return_datasets(df_test)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8608fc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16000/16000 [00:01<00:00, 14564.10 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00<00:00, 12279.40 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00<00:00, 12357.83 examples/s]\n"
     ]
    }
   ],
   "source": [
    "base_model = 'roberta-base'\n",
    "num_labels = 6\n",
    "tokenizer = RobertaTokenizer.from_pretrained(base_model)\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "\n",
    "tokenized_dataset = raw_datasets.map(tokenize_function, batched=True,  remove_columns=[\"text\"])\n",
    "train_dataset=tokenized_dataset['train']\n",
    "eval_dataset=tokenized_dataset['validation'].shard(num_shards=2, index=0)\n",
    "test_dataset=tokenized_dataset['test'].shard(num_shards=2, index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "845112ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f992550",
   "metadata": {},
   "source": [
    "### Fine-Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb8c55ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the same Training args for all models\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuning/roBERTa/\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e43f2569",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "dflabels = df[[\"label\", \"label_text\"]].drop_duplicates()\n",
    "id2label = {i: k for i, k in zip(dflabels.label.tolist(), dflabels.label_text.tolist())}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(base_model, id2label=id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c9d1be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diadochus/anaconda3/envs/hug/lib/python3.12/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da5c61b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4000' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4000/4000 05:22, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.137700</td>\n",
       "      <td>0.452463</td>\n",
       "      <td>0.893417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.292700</td>\n",
       "      <td>0.236427</td>\n",
       "      <td>0.887063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.170700</td>\n",
       "      <td>0.230061</td>\n",
       "      <td>0.905140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.156400</td>\n",
       "      <td>0.246694</td>\n",
       "      <td>0.894907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.128600</td>\n",
       "      <td>0.244754</td>\n",
       "      <td>0.897317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.120400</td>\n",
       "      <td>0.226067</td>\n",
       "      <td>0.903031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.087300</td>\n",
       "      <td>0.242911</td>\n",
       "      <td>0.903737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.089100</td>\n",
       "      <td>0.236047</td>\n",
       "      <td>0.897557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./fine_tuning/roBERTa/checkpoint-500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./fine_tuning/roBERTa/checkpoint-1000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4000, training_loss=0.14786989688873292, metrics={'train_runtime': 322.914, 'train_samples_per_second': 198.195, 'train_steps_per_second': 12.387, 'total_flos': 2725994210185728.0, 'train_loss': 0.14786989688873292, 'epoch': 4.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e524d1f6",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "12df8ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"./fine_tuning/roBERTa/checkpoint-1500/\", \n",
    "    id2label=id2label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "afe0ef79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:30<00:00, 66.36it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for _, row in tqdm(df_test.iterrows(), total=len(df_test)):\n",
    "        text = row[\"text\"]\n",
    "        encoded_input = tokenizer(text, return_tensors='pt')\n",
    "        output = model(**encoded_input)\n",
    "        ptmp = np.argmax(output.logits.detach().numpy(), axis=-1)[0]\n",
    "        predictions.append(ptmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "639649e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.assign(predictions=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3b8b4704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.96       581\n",
      "           1       0.96      0.92      0.94       695\n",
      "           2       0.78      0.92      0.84       159\n",
      "           3       0.92      0.94      0.93       275\n",
      "           4       0.95      0.81      0.87       224\n",
      "           5       0.64      0.85      0.73        66\n",
      "\n",
      "    accuracy                           0.92      2000\n",
      "   macro avg       0.87      0.90      0.88      2000\n",
      "weighted avg       0.93      0.92      0.92      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report for fine-tuned RoBERTa model. \n",
    "print(classification_report(df_test.label.tolist(), df_test.predictions.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1209cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e912ef2d",
   "metadata": {},
   "source": [
    "## T5 Fine-tuning\n",
    "\n",
    "Training - [code](https://medium.com/nlplanet/a-full-guide-to-finetuning-t5-for-text2text-and-building-a-demo-with-streamlit-c72009631887)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "98ba4c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import torch\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c488ab12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# model name\n",
    "base_model = \"t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "prefix = \"emotion classification: \"\n",
    "max_input_length = 1024\n",
    "max_target_length = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a36b5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates pytorch dataset object for emotional data\n",
    "def return_datasets(df):\n",
    "    dset = Dataset.from_dict(\n",
    "        {\n",
    "            \"label\": df.label_text.tolist(),\n",
    "            \"text\": df.text.tolist(),\n",
    "        }\n",
    "    )\n",
    "    return dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "82aa46fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    labels = tokenizer(text_target=examples[\"label\"], max_length=max_target_length, \n",
    "                       truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5666cb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes validation macro F1 score.\n",
    "metric = evaluate.load(\"f1\")\n",
    "dflabels = df[[\"label\", \"label_text\"]].drop_duplicates()\n",
    "label2id = {i: k for i, k in zip(dflabels.label_text.tolist(), dflabels.label.tolist())}\n",
    "\n",
    "\n",
    "def label_to_id(label):\n",
    "    try:\n",
    "        out_ = label2id[label]\n",
    "    except KeyError:\n",
    "        out_ = 10\n",
    "    return out_\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    decoded_preds = [label_to_id(pred.strip()) for pred in decoded_preds]\n",
    "    decoded_labels = [label_to_id(label.strip()) for label in decoded_labels]\n",
    "    \n",
    "    return metric.compute(predictions=decoded_preds, references=decoded_labels, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e7ce4313",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16000/16000 [00:00<00:00, 40081.17 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00<00:00, 30800.38 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00<00:00, 31777.08 examples/s]\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = DatasetDict(\n",
    "    {\n",
    "        \"train\": return_datasets(df),\n",
    "        \"validation\": return_datasets(df_val),\n",
    "        \"test\": return_datasets(df_test)\n",
    "    }\n",
    ")\n",
    "\n",
    "# add prefix to the text\n",
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(raw_datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de59f8b8",
   "metadata": {},
   "source": [
    "### Fine-Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3a08c6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "df8c0bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    \"./fine_tuning/t5/\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=10,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c8d21002",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bac7daf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diadochus/anaconda3/envs/hug/lib/python3.12/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4f6c6ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 17:37, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.894500</td>\n",
       "      <td>0.311162</td>\n",
       "      <td>0.624758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.327800</td>\n",
       "      <td>0.195552</td>\n",
       "      <td>0.834654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.224100</td>\n",
       "      <td>0.161413</td>\n",
       "      <td>0.863913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.182600</td>\n",
       "      <td>0.126179</td>\n",
       "      <td>0.884156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.140300</td>\n",
       "      <td>0.107012</td>\n",
       "      <td>0.893845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.126500</td>\n",
       "      <td>0.094806</td>\n",
       "      <td>0.902882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.103500</td>\n",
       "      <td>0.099679</td>\n",
       "      <td>0.896506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.106600</td>\n",
       "      <td>0.091202</td>\n",
       "      <td>0.908924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.091800</td>\n",
       "      <td>0.083066</td>\n",
       "      <td>0.911105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.087400</td>\n",
       "      <td>0.080702</td>\n",
       "      <td>0.901221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.080700</td>\n",
       "      <td>0.075558</td>\n",
       "      <td>0.908840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.078100</td>\n",
       "      <td>0.072480</td>\n",
       "      <td>0.908441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.073400</td>\n",
       "      <td>0.070313</td>\n",
       "      <td>0.907929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.067000</td>\n",
       "      <td>0.910386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.064400</td>\n",
       "      <td>0.068462</td>\n",
       "      <td>0.907704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.068000</td>\n",
       "      <td>0.067390</td>\n",
       "      <td>0.911866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.068700</td>\n",
       "      <td>0.066463</td>\n",
       "      <td>0.909928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.064100</td>\n",
       "      <td>0.064855</td>\n",
       "      <td>0.907849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.064700</td>\n",
       "      <td>0.064867</td>\n",
       "      <td>0.906741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.068700</td>\n",
       "      <td>0.064859</td>\n",
       "      <td>0.906741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diadochus/anaconda3/envs/hug/lib/python3.12/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/diadochus/anaconda3/envs/hug/lib/python3.12/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/diadochus/anaconda3/envs/hug/lib/python3.12/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/diadochus/anaconda3/envs/hug/lib/python3.12/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/diadochus/anaconda3/envs/hug/lib/python3.12/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/diadochus/anaconda3/envs/hug/lib/python3.12/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./fine_tuning/t5/checkpoint-3000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/diadochus/anaconda3/envs/hug/lib/python3.12/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./fine_tuning/t5/checkpoint-3500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/diadochus/anaconda3/envs/hug/lib/python3.12/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./fine_tuning/t5/checkpoint-4000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/diadochus/anaconda3/envs/hug/lib/python3.12/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/diadochus/anaconda3/envs/hug/lib/python3.12/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/diadochus/anaconda3/envs/hug/lib/python3.12/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/diadochus/anaconda3/envs/hug/lib/python3.12/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/diadochus/anaconda3/envs/hug/lib/python3.12/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/diadochus/anaconda3/envs/hug/lib/python3.12/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/diadochus/anaconda3/envs/hug/lib/python3.12/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/diadochus/anaconda3/envs/hug/lib/python3.12/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/diadochus/anaconda3/envs/hug/lib/python3.12/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/diadochus/anaconda3/envs/hug/lib/python3.12/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/diadochus/anaconda3/envs/hug/lib/python3.12/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/diadochus/anaconda3/envs/hug/lib/python3.12/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10000, training_loss=0.1495435489654541, metrics={'train_runtime': 1057.6915, 'train_samples_per_second': 151.273, 'train_steps_per_second': 9.455, 'total_flos': 1.074405339242496e+16, 'train_loss': 0.1495435489654541, 'epoch': 10.0})"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ba0c29",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ab6f44ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts a list to list of lists\n",
    "def get_chunks(arr, chunk_size):\n",
    "    output = []\n",
    "    for i in range(0, len(arr), chunk_size):\n",
    "        output.append(arr[i:i+chunk_size])\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "72d712b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_chunks = get_chunks(df_test.text.tolist(), chunk_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5d012167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load fine-tuned model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./fine_tuning/t5/checkpoint-9000/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ec83fe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_config = GenerationConfig(\n",
    "    max_length=max_target_length,\n",
    "    do_sample=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6da98542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:44<00:00,  1.10s/it]\n"
     ]
    }
   ],
   "source": [
    "# gather predictions\n",
    "result = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for chunk in tqdm(sentences_chunks):\n",
    "        inputs = tokenizer([prefix + sentence for sentence in chunk], return_tensors=\"pt\", padding=True)\n",
    "        # inputs.to(device)\n",
    "\n",
    "        output_sequences = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            generation_config=gen_config,\n",
    "            # do_sample=False,  # disable sampling to test if batching affects output\n",
    "        )\n",
    "\n",
    "        result = result + tokenizer.batch_decode(output_sequences, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7b796863",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.assign(predictions=result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "06c68c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.92      0.92      0.92       275\n",
      "        fear       0.90      0.85      0.88       224\n",
      "         joy       0.96      0.93      0.94       695\n",
      "        love       0.79      0.85      0.82       159\n",
      "     sadness       0.95      0.96      0.96       581\n",
      "    surprise       0.68      0.82      0.74        66\n",
      "\n",
      "    accuracy                           0.92      2000\n",
      "   macro avg       0.87      0.89      0.88      2000\n",
      "weighted avg       0.92      0.92      0.92      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report for fine-tuned T5 model. \n",
    "print(classification_report(df_test.label_text.tolist(), df_test.predictions.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "915161c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_test.label_text.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1f1a3828",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(\n",
    "    df_test.label_text.tolist(), \n",
    "    df_test.predictions.tolist(), \n",
    "    labels=labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7875e490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "disp = ConfusionMatrixDisplay(confusion_matrix=cf_matrix, display_labels=labels) \n",
    "disp.plot() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94339d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hug]",
   "language": "python",
   "name": "conda-env-hug-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
